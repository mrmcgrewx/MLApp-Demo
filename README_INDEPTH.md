# MLApp Demo

## The Idea
This app builds off of the Demo App design pattern, [which you can find here](https://github.com/mrmcgrewx/DemoApp). This app has no real purpose,
just playing around with the Swift CoreML library to generate ideas on how it could be applied within applications. Since machine-learning will
be around to stay, let us make sure we do not get left behind.

The premise of the app is simple. The user is greeted with a login screen, where they login (server code can be found here) and then can choose to
identify objects with their device's camera or review the past images they have taken. Upon selecting the capture option, the app
will take the user to a screen that then starts identifying objects in its view. Using the VGG16 Model provided by Apple, when the application
gets a confidence rating of over 50% from the model analysis, an image will be captured, then the user will be presented with the option to scrap
said image or send the data to an external server.

So lets go over how it's all put together.

## Login View
So if you read my overview on the app coordinator design pattern and network dispatcher, you will see how easy it is for us to incorporate new views
into the app and keep them independent of each other. Most of the action in the `LoginController` happens in the lines of code below:
```Swift
entryService?.login(username: user!, password: pass!, completion: { result in
            self.progressHUD.hide()
            switch result {
            case .data(let userInfo):
                self.onLoginComplete?(userInfo)
            case .error(let error):
                self.showError(error)
            }
        })
```
We are simply sending the username and password submitted by the user. If an error occurs on login, an alert displaying the error appears.
If successful, the server will return a java web token (jwt) that we will use later on to send data to the API. We pass in the data returned
from the server (which is a UserInfo object)  the `self.onLoginComplete()`, and then are taken to the home

## Home View
Now in the home view its pretty straightforward, the user has the option to either start scanning objects or review past images.
Now, taking a look at the `HomeController` class we can see how the app coordinator design pattern really shines, abstracting transition to simple functions. We can set the code to transition to the other views in the application in one liners like so:
```Swift
//MARK: Actions
    @IBAction func showCameraAction(_ sender: FancyButton) {
        showCamera?(userInfo)
    }

    @IBAction func showReviewAction(_ sender: FancyButton) {
        showReview?()
    }
```

## Camera View
The `CameraController` is the UIViewController for the Camera View and where the CoreML action is happening. The controller uses the `AVCaptureSession` object to capture video data to be processed by the CoreML model. The app also uses `CLLocationManager` to collect information on where the object was scanned.
Let's take a look at how the CoreML library is being used within the captureOutput(output:didOutputSampleBuffer:fromConnection:) delegate function. So first we set the model like so:
```Swift
guard let model = try? VNCoreMLModel(for: VGG16().model) else { return }
```
In this example we are using the VGG16 model, but you can use any of the [open source models provided by Apple](https://developer.apple.com/machine-learning/).
Now that we have the model we can now get the NVImageBasedRequest object containing the model result data:
```Swift
let request = VNCoreMLRequest(model: model) {
            (finishedReq, err) in
```
Within this request we want to get the collection of VNObservations generated by the processing of the request.
```Swift
guard let results = finishedReq.results as? [VNClassificationObservation] else { return }
```
With the classifications in hand, we now only want the top results, so we grab the first two classifications in the array.
```Swift
let topClassifications = results.prefix(2)
let descriptions = topClassifications.map { classification in
  return String(format: "  (%.2f) %@", classification.confidence, classification.identifier)
}
self.classification = descriptions
```
As you can see, its pretty straightforward to incorporate CoreML into an application.
